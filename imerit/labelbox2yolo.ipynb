{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410b3151-72f5-49e7-864f-fa557eba4615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===ONTOLOGY DETAILS===\n",
      "Name:  WHOI-RSI-USVI-Fish\n",
      "\n",
      "===PROJECT DETAILS===\n",
      "Name:  WHOI-RSI-USVI-Fish-detect-and-track\n",
      "\n",
      "===DATASET DETAILS===\n",
      "Name:  imerit-26102023-3fps-clips\n"
     ]
    }
   ],
   "source": [
    "import labelbox as lb\n",
    "import labelbox.types as lb_types\n",
    "import uuid\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "# Setup client\n",
    "with open(\"labelbox_api_key.txt\",\"r\") as f:\n",
    "    API_KEY = f.read().strip()\n",
    "client = lb.Client(api_key=API_KEY)\n",
    "\n",
    "# Get ontology\n",
    "print(\"===ONTOLOGY DETAILS===\")\n",
    "ontology = client.get_ontology(\"clqo6bd8v0jc407ybc1r9ehlb\")\n",
    "print(\"Name: \", ontology.name)\n",
    "tools = ontology.tools()\n",
    "\n",
    "# for tool in tools:\n",
    "#   print(tool)\n",
    "\n",
    "# Get project\n",
    "print(\"\\n===PROJECT DETAILS===\")\n",
    "PROJECT_ID = 'clqoh3ylw1o8s070hd6ch5z7o' # WHOI RSI USVI Fish\n",
    "# PROJECT_ID = 'clqo7auln0mpo07wphorp0t2e' # Test WHOI RSI USVI Fish\n",
    "project = client.get_project(PROJECT_ID)\n",
    "print(\"Name: \", project.name)\n",
    "\n",
    "# Get dataset\n",
    "DATASET_ID = \"clqh7v7qi001r07886j6aws7i\"\n",
    "dataset = client.get_dataset(DATASET_ID)\n",
    "print(\"\\n===DATASET DETAILS===\")\n",
    "print(\"Name: \", dataset.name)\n",
    "\n",
    "# Dataset parameters\n",
    "species_level = True # Extract species-level data (otherwise single-class \"fish\"), use with data_rows_done_only\n",
    "data_rows_done_only = True # Only utilize data rows that have undergone species review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075b028c-34d6-4f40-b9e8-f7ddfc2cfe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  104\n"
     ]
    }
   ],
   "source": [
    "# Enumerate species labels for YOLO class formatting\n",
    "classes_option = {}\n",
    "classes_enum = {}\n",
    "ordered_class_names = []\n",
    "\n",
    "classes_option[\"fish\"] = {\"label\": \"Fish\", \"value\": \"fish\"}\n",
    "classes_enum[\"fish\"] = 0\n",
    "ordered_class_names.append(\"fish\")\n",
    "\n",
    "if species_level:\n",
    "    for option_num, option in enumerate(tools[0].classifications[0].options):\n",
    "        classes_option[option.value] = option\n",
    "        classes_enum[option.value] = len(ordered_class_names)\n",
    "        ordered_class_names.append(option.value)\n",
    "print(\"Classes: \", len(ordered_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6fe507-3cae-42e0-b7d2-402e83a2354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [17:40, 299.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  Summer2016/Yawzi30mTransects060816/P6080008_1m_0s_aws160.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [38:00, 339.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  Summer2016/Tektite30mTransects060816/P6080052_0m_20s_aws157.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [38:01, 156.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  Summer2016/TEK10mTransects060716/P6070012_0m_7s_aws156.mp4\n",
      "Skipping not done:  Summer2016/TEK10mTransects060716/P6070010_0m_18s_aws155.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [41:31, 91.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  Summer2016/TEK10mTransects060716/P6070006_0m_10s_aws153.mp4\n",
      "Skipping not done:  Summer2016/TEK10mTransects060716/P6070004_0m_22s_aws152.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [42:09, 77.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  Summer2016/JoelsShoal30mTransects061016/P6100005_0m_20s_aws150.mp4\n",
      "Skipping not done:  Summer2016/JoelsShoal30mTransects061016/P6100001_0m_10s_aws149.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [54:37, 148.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  October2016/YawziOct2016/PA230109_0m_8s_aws145.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [58:26, 106.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  October2016/YawziOct2016/PA230107_0m_9s_aws143.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [1:07:30, 130.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping not done:  October2016/TektiteOct2016/PA210011_0m_4s_aws140.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [1:14:24, 186.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dry_run:\n\u001b[1;32m     71\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_img_path\u001b[38;5;241m.\u001b[39mparent, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 72\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy2(img_path, output_img_path)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCopying \u001b[39m\u001b[38;5;124m\"\u001b[39m, img_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m, output_img_path)\n",
      "File \u001b[0;32m/media/data/anaconda3_envs/aws_labelbox/lib/python3.11/shutil.py:436\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    435\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 436\u001b[0m copyfile(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    437\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/media/data/anaconda3_envs/aws_labelbox/lib/python3.11/shutil.py:258\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    261\u001b[0m                 \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Recommended to download JSON from Labelbox using the Browser Interface\n",
    "# TODO: Verify whether or not this includes interpolated (non-keyframed) data\n",
    "\n",
    "# Extracts fish labels from labelbox json file and converts them into YOLO format\n",
    "# Assumes the global_key from labelbox matches the directory structure of the images\n",
    "# Fish class is assumed as 0\n",
    "\n",
    "dry_run = False\n",
    "save_images = False\n",
    "make_copy = True # Good for creating sub-datasets (like species-classifier, since not all videos have been labelled to that level)\n",
    "\n",
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "from pathlib import Path\n",
    "from bbox_utils import *\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "json_path = \"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-labels-04042024.json\"\n",
    "image_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-yolo-dataset/images\")\n",
    "# label_output_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-yolo-dataset/labels\")\n",
    "# label_output_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-yolo-dataset/labels_species_only\")\n",
    "# label_output_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-yolo-dataset/test_labels\")\n",
    "# image_output_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-yolo-dataset/test_images\")\n",
    "\n",
    "label_output_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-species-yolo-dataset/test_labels\")\n",
    "image_output_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-species-yolo-dataset/test_images\")\n",
    "\n",
    "list_of_videos = []\n",
    "stats = {}\n",
    "\n",
    "# with open(json_path, \"r\") as f:\n",
    "with jsonlines.open(json_path, \"r\") as f:\n",
    "\n",
    "    # Iterate through each video in the JSON file\n",
    "    for i, datarow in tqdm(enumerate(f)):\n",
    "        global_key = datarow[\"data_row\"][\"global_key\"]\n",
    "        \n",
    "        # Skip datarows that are not DONE, if applicable (usually used alongside species-level)\n",
    "        project_status = datarow[\"projects\"][PROJECT_ID][\"project_details\"][\"workflow_status\"]\n",
    "        if data_rows_done_only and not project_status == \"DONE\":\n",
    "            print(\"Skipping not done: \", global_key)\n",
    "            continue\n",
    "\n",
    "        img_sz = (datarow[\"media_attributes\"][\"width\"], datarow[\"media_attributes\"][\"height\"])\n",
    "        \n",
    "        # Video path\n",
    "        vid_path = Path(global_key)\n",
    "        rel_vid_path = vid_path.parent / \"_\".join(vid_path.stem.split(\"_\")[:-1])\n",
    "\n",
    "        # Grab frame labels\n",
    "        try:\n",
    "            frames_json = datarow[\"projects\"][PROJECT_ID][\"labels\"][0][\"annotations\"][\"frames\"]\n",
    "        except:\n",
    "            print(\"Skipping \", global_key, \" has no labels\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through frames\n",
    "        frame_count = datarow[\"media_attributes\"][\"frame_count\"]\n",
    "        \n",
    "        for frame_id in range(frame_count):\n",
    "            img_name = \"frame_%03d\"%(int(frame_id))\n",
    "            img_path = image_root / rel_vid_path / (img_name + \".png\")\n",
    "            output_path = label_output_root / rel_vid_path / (img_name + \".txt\")  \n",
    "\n",
    "            if make_copy:\n",
    "                output_img_path = image_output_root / rel_vid_path / (img_name + \".png\")\n",
    "\n",
    "                if not dry_run:\n",
    "                    os.makedirs(output_img_path.parent, exist_ok=True)\n",
    "                    shutil.copy2(img_path, output_img_path)\n",
    "                else:\n",
    "                    print(\"Copying \", img_path, \" to \", output_img_path)\n",
    "                \n",
    "            # Verify this image exists\n",
    "            assert img_path.exists(), f\"Image not found {img_path}\"\n",
    "            \n",
    "            os.makedirs(output_path.parent, exist_ok=True)\n",
    "            \n",
    "            # Make label file, overwrite if already there\n",
    "            open(output_path, \"w\")\n",
    "\n",
    "            # No labels in this video, so continue\n",
    "            if str(frame_id) in frames_json:\n",
    "                frame_data = frames_json[str(frame_id)]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for object_id, object_data in frame_data[\"objects\"].items():\n",
    "                lbl_bbox = object_data[\"bounding_box\"]\n",
    "                \n",
    "                if species_level:\n",
    "                    if len(object_data[\"classifications\"]) > 0:\n",
    "                        class_name = object_data[\"classifications\"][0][\"radio_answer\"][\"value\"]\n",
    "                        label = classes_enum[class_name]\n",
    "                    else:\n",
    "                        label = 0\n",
    "                else:\n",
    "                    label = 0\n",
    "\n",
    "                yolo_bbox = list(labelbox2yolo_bbox(lbl_bbox, img_sz))\n",
    "                yolo_bbox.insert(0, label) # Fish class for now\n",
    "                with open(output_path, \"a\") as f:\n",
    "                    if not dry_run:\n",
    "                        f.write(\" \".join(map(str, yolo_bbox)))\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "        list_of_videos.append(global_key)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "466131ed-d753-4e32-8006-4fd72968d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcc5aa70-c67b-4a95-b276-1f265d478681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create yolov5 dataset configuration yaml\n",
    "import yaml\n",
    "\n",
    "project_root = Path(\"/srv/warplab/shared/datasets/WHOI_RS_Fish_Detector/whoi-rsi-fish-detection-yolo-dataset/\")\n",
    "train_split_filename = \"train_species_split.txt\"\n",
    "val_split_filename = \"val_species_split.txt\"\n",
    "test_split_filename = \"test_species_split.txt\"\n",
    "dataset_yaml_filename = \"fish_species_yolo_dataset.yaml\"\n",
    "\n",
    "if species_level:\n",
    "    names = dict(zip(classes_enum.values(), classes_enum.keys()))\n",
    "else:\n",
    "    names = {0: \"fish\"}\n",
    "\n",
    "yolo_dataset = {\n",
    "    \"path\": str(project_root),\n",
    "    \"train\": f\"./{train_split_filename}\",\n",
    "    \"val\": f\"./{val_split_filename}\",\n",
    "    \"test\": f\"./{test_split_filename}\",\n",
    "    \"names\": names,\n",
    "}\n",
    "yaml_data = yaml.dump(yolo_dataset)\n",
    "with open(project_root / dataset_yaml_filename, \"w\") as f:\n",
    "    f.write(yaml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ba7239b-36bd-4a86-8e72-7cd88aef2f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14670it [04:13, 57.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'num_train_frames': 1890, 'num_val_frames': 630, 'num_test_frames': 3240}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create train, val, and test splits\n",
    "open(project_root / train_split_filename, \"w\")\n",
    "open(project_root / val_split_filename, \"w\")\n",
    "open(project_root / test_split_filename, \"w\")\n",
    "\n",
    "import glob\n",
    "\n",
    "# Test split contains only years 2016 and 2017, these dates are inferred from the global_key\n",
    "img_paths = glob.iglob(str(project_root / \"**/*.png\"), recursive=True)\n",
    "\n",
    "# Note: this only adds images that have corresponding labels\n",
    "split_stats = {}\n",
    "split_stats[\"num_train_frames\"] = 0\n",
    "split_stats[\"num_val_frames\"] = 0\n",
    "split_stats[\"num_test_frames\"] = 0\n",
    "\n",
    "\n",
    "for img_path in tqdm(img_paths):\n",
    "    # Get relative path starting at project root\n",
    "    project_root_parts = len(project_root.parts)\n",
    "\n",
    "    frame_path = Path(*Path(img_path).parts[project_root_parts+1:])\n",
    "    project_img_path = \"images\" / frame_path\n",
    "    \n",
    "    vid_path = project_img_path.parent\n",
    "    \n",
    "    if not (project_root / \"labels\" / frame_path.with_suffix(\".txt\")).exists():\n",
    "        continue\n",
    "    \n",
    "    with open(project_root / \"video_list.txt\", \"a\") as f:\n",
    "        f.write(str(vid_path) + \"\\n\")\n",
    "        \n",
    "    # Test split\n",
    "    if \"2016\" in str(project_img_path) or \"2017\" in str(project_img_path):\n",
    "        with open(project_root / test_split_filename, \"a\") as f:\n",
    "            f.write(\"./\" + str(project_img_path) + \"\\n\")\n",
    "        split_stats[\"num_test_frames\"] += 1\n",
    "            \n",
    "    # Val split\n",
    "    elif \"2018\" in str(project_img_path):\n",
    "        with open(project_root / val_split_filename, \"a\") as f:\n",
    "            f.write(\"./\" + str(project_img_path) + \"\\n\")\n",
    "        split_stats[\"num_val_frames\"] += 1\n",
    "    \n",
    "    # Train split\n",
    "    else:\n",
    "        with open(project_root / train_split_filename, \"a\") as f:\n",
    "            f.write(\"./\" + str(project_img_path) + \"\\n\")\n",
    "        split_stats[\"num_train_frames\"] += 1\n",
    "        \n",
    "print(\"done\")\n",
    "print(split_stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
